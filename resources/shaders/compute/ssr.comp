layout(std140, binding = 1) uniform UniformBlock
{
	float zThickness; // How thick is each depth fragment? higher value yields some wierd smudging at edges of reflection, but thinner z means we might miss some geometry. This should essentially be the average thickness of the geometry. to do dynamically would be a nightmare however...
	float stride; // lets you skip pixels during iteration. Larger stride means longer rays with same number of samples.
	float jitter; // if stride > 1 we ge banding results. This can be traded for noise by jittering the start position of each ray.
	float maxSteps; //Less steps means shorter reflections, but better performance
	float maxDistance; //Not orthogonal to max steps. reflection of surfaces far away in world space are prone to rapid shifts with only a slight change in camera positioning, which can lead to objectionable temporal flicker. Setting this parameter can help mitigate that.
	uvec2 workGroups; // How many workgroups do we have?
	float padding; // Just padding. Might not be needed because std140
};

#define THREADS_X 16
#define THREADS_Y 16

// All the samplers we need!
uniform sampler2D depthMap;
uniform sampler2D normalMap;
uniform sampler2D specularMap;
uniform sampler2D colorMap;

// Shared values between all the threads in a group
//shared DepthSamples

layout (rgba32f) uniform writeonly image2D reflectionImage;

float distanceSquared(vec2 a, vec2 b)
{
	a -= b;
	return dot(a, a);
}

bool TraceScreenSpaceRay(in vec3 rayOrigin,
						 in vec3 rayDirection,
						 out vec2 hitTexCoord,
						 out vec3 hitPoint
						 )
{
	float nearPlane = -0.05f;

	// Clip to the near plane
	float rayLength = ((rayOrigin.z + rayDirection.z * maxDistance) > nearPlane) ? (nearPlane - rayOrigin.z) / rayDirection.z : maxDistance;

	vec3 rayEnd = rayOrigin + rayDirection * rayLength;

	float sx = float(ScreenSize.x) * 0.5f;
	float sy = float(ScreenSize.y) * 0.5f;

	mat4 scrScale = mat4(sx, 0.0f, 0.0f, 0.0f,
						 0.0f, sy, 0.0f, 0.0f,
						 0.0f, 0.0f, 1.0f, 0.0f,
						 sx, sy, 0.0f, 1.0f);

	mat4 proj = (scrScale * Projection);

	// Project into homogeneous clip space
	vec4 H0 = proj * vec4( rayOrigin, 1.0f);
	vec4 H1 = proj * vec4( rayEnd, 1.0f);

	float k0 = 1.0 / H0.w, k1 = 1.0 / H1.w;

	// The interpolated homogeneous version of the camera-space points
	vec3 Q0 = rayOrigin * k0, Q1 = rayEnd * k1;

	// Screen-space endpoints
	vec2 P0 = H0.xy * k0, P1 = H1.xy * k1;

	// If the line is degenerate, make it cover at least one pixel
	// to avoid handling zero-pixel extent as a special case later
	P1 += (distanceSquared(P0, P1) < 0.0001) ? 0.01 : 0.0;

	vec2 delta = P1 - P0;

	// Permute so that the primary iteration is in x to collapse
	// all quadrant-specific DDA cases later
	bool permute = false;

	if (abs(delta.x) < abs(delta.y))
	{
		// This is a more-vertical line
		permute = true; delta = delta.yx; P0 = P0.yx; P1 = P1.yx;
	}

	float stepDir = sign(delta.x);
	float invdx = stepDir / delta.x;

	// Track the derivatives of Q and k
	vec3  dQ = (Q1 - Q0) * invdx;
	float dk = (k1 - k0) * invdx;
	vec2  dP = vec2(stepDir, delta.y * invdx);

	// Calculate pixel stride based on distance of ray origin from camera.
	// Since perspective means distant objects will be smaller in screen space
	// we can use this to have higher quality reflections for far away objects
	// while still using a large pixel stride for near objects (and increase performance)
	// this also helps mitigate artifacts on distant reflections when we use a large
	// pixel stride.
	//float strideScaler = 1.0 + min( 1.0, -rayOrigin.z / 100.0f);
	//float pixelStride = 1.0 + strideScaler * stride;


	// Scale derivatives by the desired pixel stride and then
	// offset the starting values by the jitter fraction
	dP *= stride;
	dQ *= stride;
	dk *= stride;
	P0 += dP * jitter;
	Q0 += dQ * jitter;
	k0 += dk * jitter;

	float i, zA = 0.0, zB = 0.0;

	// Track ray step and derivatives in a float4 to parallelize
	vec4 pqk = vec4( P0, Q0.z, k0);
	vec4 dPQK = vec4( dP, dQ.z, dk);
	bool intersect = false;

	float end = P1.x * stepDir;

	for(i=0; i < maxSteps && intersect == false; i++)
	{
		pqk += dPQK;

		zA = zB;
		zB = (dPQK.z * 0.5 + pqk.z) / (dPQK.w * 0.5 + pqk.w);
		if (zB > zA)
		{
			//Swap
			float t = zA;
			zA = zB;
			zB = t;
        }

		hitTexCoord = permute ? pqk.yx : pqk.xy;

		if(hitTexCoord.x > ScreenSize.x || hitTexCoord.y > ScreenSize.y || hitTexCoord.x < 0 || hitTexCoord.y < 0 || pqk.x * stepDir>= end)
		{
			break;
		}

		float depth = -texelFetch(depthMap, ivec2(hitTexCoord), 0).r;

		//intersect = (zB <= depth);

		intersect = (zB >= depth - zThickness) && (zA <= depth);
	}
	/*
	// Binary search refinement
	if( pixelStride > 1.0 && intersect)
	{
		pqk -= dPQK;
		dPQK /= pixelStride;

		float originalStride = pixelStride * 0.5;
		float stride = originalStride;
		zA = pqk.z / pqk.w;
		zB = zA;
		float j;
		for(j=0; j< 64.0f; j++)
		{
			pqk += dPQK * stride;
			zA = zB;
			zB = (dPQK.z * -0.5 + pqk.z) / (dPQK.w * -0.5 + pqk.w);
			if (zB > zA)
			{
				//Swap
				float t = zA;
				zA = zB;
				zB = t;
			}

			hitTexCoord = permute ? pqk.yx : pqk.xy;

			originalStride *= 0.5;
			stride = (zB <= (-texelFetch(depthMap, ivec2(hitTexCoord), 0).r)) ? -originalStride : originalStride;
		}
	}
	*/

	Q0.xy += dQ.xy * i;
	Q0.z = pqk.z;
	hitPoint = Q0 / pqk.w;
	//iterationCount = i;

	return intersect;
}


layout(local_size_x = THREADS_X, local_size_y = THREADS_Y, local_size_z = 1) in;
void main()
{
	ivec2 location = ivec2(gl_GlobalInvocationID.xy);

	//Per pixel on depthbuffer

	vec2 TexCoord = vec2(location) / ScreenSize;
	//First of all we need to generate a reflection vector that we can raytrace

	//Get normal from normal map. This only needs to be done once per pixel
	vec3 worldNormal = (texelFetch(normalMap, location, 0).rgb);

	//We need the normal to be in viewspace coordinates.
	vec3 viewSpaceNormal = normalize((View * vec4(worldNormal, 0.0f)).rgb);

	float pixelDepth = texelFetch(depthMap, location, 0).r;

	//Calculate world pos
	vec4 clipSpaceLocation;
	clipSpaceLocation.xy = TexCoord * 2.0f - 1.0f;
	clipSpaceLocation.z = 1.0f;
	clipSpaceLocation.w = 1.0f;
	vec4 homogenousLocation = InvProjection * clipSpaceLocation;
	vec3 viewSpacePosition = homogenousLocation.xyz;

	vec3 rayOrigin = viewSpacePosition * pixelDepth;

	vec3 viewDir = (rayOrigin.xyz);

	//Reflect vector against normal
	vec3 reflectionDir = (reflect(viewDir, viewSpaceNormal));//normalize(-2 * dot(viewDir, viewSpaceNormal) * viewSpaceNormal + viewDir);

	vec2 hitTexCoord = vec2(0.0,0.0);
	vec3 hitPoint = vec3(0.0,0.0,0.0);

	//vec2 uv2 = vec2(location);
	//float c = (uv2.x + uv2.y) * jitter;//0.25;
	//float jitterFraction = c - 1.0f * floor(c/1.0f);

	if (TraceScreenSpaceRay(rayOrigin.xyz, reflectionDir, hitTexCoord, hitPoint))
	{
		//hitpoint is in viewspace. Convert to texture coordinates.
		//vec4 clipspace = Projection * vec4(hitPoint.xyz, 1.0f);

		//vec2 coord = ((clipspace.xy / clipspace.w) + 1.0f) * 0.5f;
		//coord = coord * ScreenSize;

		vec3 reflectionColor = texelFetch(colorMap, ivec2(hitTexCoord), 0).rgb;
		vec4 specularAndRoughness = texelFetch(specularMap, location, 0).rgba;

		float roughness = specularAndRoughness.w;

		vec3 BGColor = texelFetch(colorMap, location, 0).rgb;

		//vec4 finalColor = vec4(BGColor, roughness) + vec4(reflectionColor.rgb * (specularAndRoughness.rgb * 0.5f) * roughness, 1-roughness);

		vec4 finalColor = vec4(reflectionColor.rgb, 1.0f);

		imageStore(reflectionImage, location, finalColor);
	}
	else
	{
		//Get fall-back value.
		//vec3 BGColor = texelFetch(colorMap, location, 0).rgb;
		vec3 BGColor = vec3(0.0f,0.0f,0.0f);
		imageStore(reflectionImage, location, vec4(BGColor,1.0f));
	}

	groupMemoryBarrier();
}
